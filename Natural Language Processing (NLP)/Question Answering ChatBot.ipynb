{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Question Answering ChatBot.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1XFiX7YAUGiu46jWwbU2wI7hAcwrfCx-k","authorship_tag":"ABX9TyPsmXNWiF1S+KuPgzfCTHxa"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"dMFZfjNfI5HB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622112260372,"user_tz":-330,"elapsed":1753,"user":{"displayName":"Ravi Gautam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgkLZy1a2u6YJBCUU4_udtV8zSehPd66tBnKMZ5=s64","userId":"04156610004868127973"}},"outputId":"f8f4caab-ba8b-4186-9199-4ac01a91585b"},"source":["import nltk\n","import numpy as np\n","from json import load\n","from nltk.stem.porter import PorterStemmer\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset,DataLoader\n","from google.colab.files import upload\n","nltk.download('punkt')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"z1T3vXV4hQYo"},"source":["## Preprocessing metods are defined like stemming and Tokenization. Bag of words is defined to convert in vector form"]},{"cell_type":"code","metadata":{"id":"OvsmjKawK_Kb","executionInfo":{"status":"ok","timestamp":1622112274587,"user_tz":-330,"elapsed":399,"user":{"displayName":"Ravi Gautam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgkLZy1a2u6YJBCUU4_udtV8zSehPd66tBnKMZ5=s64","userId":"04156610004868127973"}}},"source":["stemmer=PorterStemmer()\n","def tokenize(sentence):\n","  return nltk.word_tokenize(sentence)\n","def stem(word):\n","  return stemmer.stem(word.lower())\n","def bag_of_words(tokenized_sentence,all_words):\n","  tokenized_sentence=[stem(w) for w in tokenized_sentence]\n","  bag=np.zeros(len(all_words),dtype=np.float32)\n","  for idx,w in enumerate(all_words):\n","    if w in tokenized_sentence:\n","      bag[idx]=1.0\n","  return bag"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x1Sgv8TThlh3"},"source":["## Loading of Json Data and creation of Traing Data"]},{"cell_type":"code","metadata":{"id":"CBeHl3WgNO_Y","executionInfo":{"status":"ok","timestamp":1622112279660,"user_tz":-330,"elapsed":8,"user":{"displayName":"Ravi Gautam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgkLZy1a2u6YJBCUU4_udtV8zSehPd66tBnKMZ5=s64","userId":"04156610004868127973"}}},"source":["with open('intents.json','r') as f:\n","  intents=load(f)\n","all_words=[]\n","tags=[]\n","xy=[]\n","for intent in intents['intents']:\n","  tag=intent['tag']\n","  tags.append(tag)\n","  for pattern in intent['patterns']:\n","    w=tokenize(pattern)\n","    all_words.extend(w)\n","    xy.append((w,tag))\n","ignore_words=[\"?\",\"!\",\",\",\".\"]\n","all_words=sorted(set([stem(w) for w in all_words if w not in ignore_words]))\n","x_train,y_train=[],[]\n","for (pattern_sentence,tag) in xy:\n","  bag=bag_of_words(pattern_sentence,all_words)\n","  x_train.append(bag)\n","  label=tags.index(tag)\n","  y_train.append(label)\n","x_train=np.array(x_train)\n","y_train=np.array(y_train)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bqO_NGzDhtNE"},"source":["## Data converted to batches for easy acces during training of Model using Dataset and DataLoader"]},{"cell_type":"code","metadata":{"id":"wW8XAb5UF3Bg","executionInfo":{"status":"ok","timestamp":1622112282729,"user_tz":-330,"elapsed":377,"user":{"displayName":"Ravi Gautam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgkLZy1a2u6YJBCUU4_udtV8zSehPd66tBnKMZ5=s64","userId":"04156610004868127973"}}},"source":["class ChatDataset(Dataset):\n","  def __init__(self):\n","    self.n_samples=len(x_train)\n","    self.x_data=x_train\n","    self.y_data=y_train\n","  def __getitem__(self,index):\n","    return self.x_data[index],self.y_data[index]\n","  def __len__(self):\n","    return self.n_samples\n","\n","batch_size=8\n","hidden_size=8\n","output_size=len(tags)\n","input_size=len(all_words)\n","dataset=ChatDataset()\n","train_loader=DataLoader(dataset=dataset,batch_size=batch_size,shuffle=True)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Aszvvk4Ph-bQ"},"source":["## Model building with desired architecture"]},{"cell_type":"code","metadata":{"id":"ZjGC7R4q8YoE"},"source":["class NeuralNet(nn.Module):\n","  def __init__(self,input_size,hidden_size,num_classes):\n","    super(NeuralNet,self).__init__()\n","    self.l1=nn.Linear(input_size,hidden_size)\n","    self.l2=nn.Linear(hidden_size,hidden_size)\n","    self.l3=nn.Linear(hidden_size,num_classes)\n","    self.relu=nn.ReLU()\n","  def forward(self,x):\n","    out=self.relu(self.l1(x))\n","    out=self.relu(self.l2(out))\n","    out=self.l3(out)\n","    return out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C_2KXagkiLmi"},"source":["## Training of the Model."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xcALR8jNDh3d","executionInfo":{"status":"ok","timestamp":1622108148658,"user_tz":-330,"elapsed":4862,"user":{"displayName":"Ravi Gautam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgkLZy1a2u6YJBCUU4_udtV8zSehPd66tBnKMZ5=s64","userId":"04156610004868127973"}},"outputId":"5be0f077-6819-4784-da22-e02437eaae50"},"source":["device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model=NeuralNet(input_size,hidden_size,output_size).to(device)\n","criterion=nn.CrossEntropyLoss()\n","optimizer=torch.optim.Adam(model.parameters(),lr=0.001)\n","for epoch in range(1000):\n","  for (words,labels) in train_loader:\n","    words=words.to(device)\n","    labels=labels.to(device)\n","    outputs=model(words)\n","    loss=criterion(outputs,labels)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","  if epoch%100==0:\n","    print(\"epoch={} and loss={}\".format(epoch,loss.item()))\n","print(\"final loss=\",loss)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["epoch=0 and loss=1.74021315574646\n","epoch=100 and loss=0.8956341743469238\n","epoch=200 and loss=0.21258723735809326\n","epoch=300 and loss=0.04612146317958832\n","epoch=400 and loss=0.02971796505153179\n","epoch=500 and loss=0.009276503697037697\n","epoch=600 and loss=0.004896452650427818\n","epoch=700 and loss=0.0009313729824498296\n","epoch=800 and loss=0.001419661333784461\n","epoch=900 and loss=0.00484789814800024\n","final loss= tensor(0.0006, grad_fn=<NllLossBackward>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"L0SyZ4HSiYoN"},"source":["## Saving The trained model parameters and architecture of the model along with word vocabulary and tags."]},{"cell_type":"markdown","metadata":{"id":"DS4lHZA3iyP1"},"source":["## Running and testing the trained Model."]},{"cell_type":"code","metadata":{"id":"L9h41ieByipf"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jBL_6KUGZnE8","executionInfo":{"status":"ok","timestamp":1622108280248,"user_tz":-330,"elapsed":104324,"user":{"displayName":"Ravi Gautam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgkLZy1a2u6YJBCUU4_udtV8zSehPd66tBnKMZ5=s64","userId":"04156610004868127973"}},"outputId":"80b0bcd3-09aa-47c6-c889-609fb913a0fb"},"source":["bot_name=\"RG\"\n","print(\"Let's chat: type 'quit' to exit\")\n","while True:\n","  sentence=input(\"You:\")\n","  if sentence==\"quit\":\n","    break\n","  sentence=tokenize(sentence)\n","  x=bag_of_words(sentence,all_words)\n","  x=x.reshape(1,x.shape[0])\n","  x=torch.from_numpy(x)\n","  output=model(x)\n","  _,predicted=torch.max(output,dim=1)\n","  tag=tags[predicted.item()]\n","  probs=torch.softmax(output,dim=1)\n","  prob=probs[0][predicted.item()]\n","  if prob.item()>0.75:\n","    for intent in intents[\"intents\"]:\n","      if tag==intent[\"tag\"]:\n","        response=np.random.choice(intent[\"responses\"])\n","        print(\"{}: {}\".format(bot_name,response))\n","    if tag==\"goodbye\":\n","      break\n","  else:\n","    print(\"{}: I do not understand...\".format(bot_name))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Let's chat: type 'quit' to exit\n","You:hi\n","RG: Hey :-)\n","You:when can you deliver\n","RG: I do not understand...\n","You:how long it takes you to deliver\n","RG: Delivery takes 2-4 days\n","You:how can i pay you\n","RG: I do not understand...\n","You:do you take cash\n","RG: We accept most major credit cards, and Paypal\n","You:ok bye\n","RG: Have a nice day\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V1i7RNaM7o0e"},"source":[""],"execution_count":null,"outputs":[]}]}
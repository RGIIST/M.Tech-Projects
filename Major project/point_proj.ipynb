{"cells":[{"cell_type":"markdown","metadata":{"id":"iKd3yMJh2DNN"},"source":["## Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zm5f0MoT2BBu"},"outputs":[],"source":["import copy\n","from typing import Optional, List\n","\n","import torch\n","import torch.nn.functional as F\n","from torch import nn, Tensor\n","\n","\n","class Transformer(nn.Module):\n","\n","    def __init__(self, d_model=512, nhead=8, num_encoder_layers=6,\n","                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1,\n","                 activation=\"relu\", normalize_before=False,\n","                 return_intermediate_dec=False):\n","        super().__init__()\n","\n","        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward,\n","                                                dropout, activation, normalize_before)\n","        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n","        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n","\n","        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward,\n","                                                dropout, activation, normalize_before)\n","        decoder_norm = nn.LayerNorm(d_model)\n","        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm,\n","                                          return_intermediate=return_intermediate_dec)\n","        self.output_layer=nn.Linear(dim_feedforward,4) #num_classes=4\n","\n","        self._reset_parameters()\n","\n","        self.d_model = d_model\n","        self.nhead = nhead\n","\n","    def _reset_parameters(self):\n","        for p in self.parameters():\n","            if p.dim() > 1:\n","                nn.init.xavier_uniform_(p)\n","\n","    def forward(self, src, mask, query_embed, pos_embed=None):\n","        # flatten NxCxHxW to HWxNxC\n","        bs, h, w = src.shape\n","        # src = src.flatten(1).permute(2, 0, 1)\n","        src=src.view(-1,1,3)\n","        # pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n","        # query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)\n","        # mask = mask.flatten(1)\n","\n","        tgt = torch.zeros(1,3)\n","        # tgt = torch.zeros_like(query_embed)\n","        memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)\n","        hs = self.decoder(tgt, memory, memory_key_padding_mask=mask,\n","                          pos=pos_embed, query_pos=query_embed)\n","        output=self.output_layer(hs)\n","        # return hs.transpose(1, 2), memory.permute(1, 2, 0).view(bs, c, h, w)\n","        return nn.Softmax(output)\n","\n","\n","class TransformerEncoder(nn.Module):\n","\n","    def __init__(self, encoder_layer, num_layers, norm=None):\n","        super().__init__()\n","        self.layers = _get_clones(encoder_layer, num_layers)\n","        self.num_layers = num_layers\n","        self.norm = norm\n","\n","    def forward(self, src,\n","                mask: Optional[Tensor] = None,\n","                src_key_padding_mask: Optional[Tensor] = None,\n","                pos: Optional[Tensor] = None):\n","        output = src\n","\n","        for layer in self.layers:\n","            output = layer(output, src_mask=mask,\n","                           src_key_padding_mask=src_key_padding_mask, pos=pos)\n","\n","        if self.norm is not None:\n","            output = self.norm(output)\n","\n","        return output\n","\n","\n","class TransformerDecoder(nn.Module):\n","\n","    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False):\n","        super().__init__()\n","        self.layers = _get_clones(decoder_layer, num_layers)\n","        self.num_layers = num_layers\n","        self.norm = norm\n","        self.return_intermediate = return_intermediate\n","\n","    def forward(self, tgt, memory,\n","                tgt_mask: Optional[Tensor] = None,\n","                memory_mask: Optional[Tensor] = None,\n","                tgt_key_padding_mask: Optional[Tensor] = None,\n","                memory_key_padding_mask: Optional[Tensor] = None,\n","                pos: Optional[Tensor] = None,\n","                query_pos: Optional[Tensor] = None):\n","        output = tgt\n","\n","        intermediate = []\n","\n","        for layer in self.layers:\n","            output = layer(output, memory, tgt_mask=tgt_mask,\n","                           memory_mask=memory_mask,\n","                           tgt_key_padding_mask=tgt_key_padding_mask,\n","                           memory_key_padding_mask=memory_key_padding_mask,\n","                           pos=pos, query_pos=query_pos)\n","            if self.return_intermediate:\n","                intermediate.append(self.norm(output))\n","\n","        if self.norm is not None:\n","            output = self.norm(output)\n","            if self.return_intermediate:\n","                intermediate.pop()\n","                intermediate.append(output)\n","\n","        if self.return_intermediate:\n","            return torch.stack(intermediate)\n","\n","        return output.unsqueeze(0)\n","\n","\n","class TransformerEncoderLayer(nn.Module):\n","\n","    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n","                 activation=\"relu\", normalize_before=False):\n","        super().__init__()\n","        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n","        # Implementation of Feedforward model\n","        self.linear1 = nn.Linear(d_model, dim_feedforward)\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear2 = nn.Linear(dim_feedforward, d_model)\n","\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.dropout1 = nn.Dropout(dropout)\n","        self.dropout2 = nn.Dropout(dropout)\n","\n","        self.activation = _get_activation_fn(activation)\n","        self.normalize_before = normalize_before\n","\n","    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n","        return tensor if pos is None else tensor + pos\n","\n","    def forward_post(self,\n","                     src,\n","                     src_mask: Optional[Tensor] = None,\n","                     src_key_padding_mask: Optional[Tensor] = None,\n","                     pos: Optional[Tensor] = None):\n","        q = k = self.with_pos_embed(src, pos)\n","        src2 = self.self_attn(q, k, value=src, attn_mask=src_mask,\n","                              key_padding_mask=src_key_padding_mask)[0]\n","        src = src + self.dropout1(src2)\n","        src = self.norm1(src)\n","        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n","        src = src + self.dropout2(src2)\n","        src = self.norm2(src)\n","        return src\n","\n","    def forward_pre(self, src,\n","                    src_mask: Optional[Tensor] = None,\n","                    src_key_padding_mask: Optional[Tensor] = None,\n","                    pos: Optional[Tensor] = None):\n","        src2 = self.norm1(src)\n","        q = k = self.with_pos_embed(src2, pos)\n","        src2 = self.self_attn(q, k, value=src2, attn_mask=src_mask,\n","                              key_padding_mask=src_key_padding_mask)[0]\n","        src = src + self.dropout1(src2)\n","        src2 = self.norm2(src)\n","        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n","        src = src + self.dropout2(src2)\n","        return src\n","\n","    def forward(self, src,\n","                src_mask: Optional[Tensor] = None,\n","                src_key_padding_mask: Optional[Tensor] = None,\n","                pos: Optional[Tensor] = None):\n","        if self.normalize_before:\n","            return self.forward_pre(src, src_mask, src_key_padding_mask, pos)\n","        return self.forward_post(src, src_mask, src_key_padding_mask, pos)\n","\n","\n","class TransformerDecoderLayer(nn.Module):\n","\n","    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n","                 activation=\"relu\", normalize_before=False):\n","        super().__init__()\n","        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n","        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n","        # Implementation of Feedforward model\n","        self.linear1 = nn.Linear(d_model, dim_feedforward)\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear2 = nn.Linear(dim_feedforward, d_model)\n","\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.norm3 = nn.LayerNorm(d_model)\n","        self.dropout1 = nn.Dropout(dropout)\n","        self.dropout2 = nn.Dropout(dropout)\n","        self.dropout3 = nn.Dropout(dropout)\n","\n","        self.activation = _get_activation_fn(activation)\n","        self.normalize_before = normalize_before\n","\n","    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n","        return tensor if pos is None else tensor + pos\n","\n","    def forward_post(self, tgt, memory,\n","                     tgt_mask: Optional[Tensor] = None,\n","                     memory_mask: Optional[Tensor] = None,\n","                     tgt_key_padding_mask: Optional[Tensor] = None,\n","                     memory_key_padding_mask: Optional[Tensor] = None,\n","                     pos: Optional[Tensor] = None,\n","                     query_pos: Optional[Tensor] = None):\n","        q = k = self.with_pos_embed(tgt, query_pos)\n","        tgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask,\n","                              key_padding_mask=tgt_key_padding_mask)[0]\n","        tgt = tgt + self.dropout1(tgt2)\n","        tgt = self.norm1(tgt)\n","        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),\n","                                   key=self.with_pos_embed(memory, pos),\n","                                   value=memory, attn_mask=memory_mask,\n","                                   key_padding_mask=memory_key_padding_mask)[0]\n","        tgt = tgt + self.dropout2(tgt2)\n","        tgt = self.norm2(tgt)\n","        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n","        tgt = tgt + self.dropout3(tgt2)\n","        tgt = self.norm3(tgt)\n","        return tgt\n","\n","    def forward_pre(self, tgt, memory,\n","                    tgt_mask: Optional[Tensor] = None,\n","                    memory_mask: Optional[Tensor] = None,\n","                    tgt_key_padding_mask: Optional[Tensor] = None,\n","                    memory_key_padding_mask: Optional[Tensor] = None,\n","                    pos: Optional[Tensor] = None,\n","                    query_pos: Optional[Tensor] = None):\n","        tgt2 = self.norm1(tgt)\n","        q = k = self.with_pos_embed(tgt2, query_pos)\n","        tgt2 = self.self_attn(q, k, value=tgt2, attn_mask=tgt_mask,\n","                              key_padding_mask=tgt_key_padding_mask)[0]\n","        tgt = tgt + self.dropout1(tgt2)\n","        tgt2 = self.norm2(tgt)\n","        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt2, query_pos),\n","                                   key=self.with_pos_embed(memory, pos),\n","                                   value=memory, attn_mask=memory_mask,\n","                                   key_padding_mask=memory_key_padding_mask)[0]\n","        tgt = tgt + self.dropout2(tgt2)\n","        tgt2 = self.norm3(tgt)\n","        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n","        tgt = tgt + self.dropout3(tgt2)\n","        return tgt\n","\n","    def forward(self, tgt, memory,\n","                tgt_mask: Optional[Tensor] = None,\n","                memory_mask: Optional[Tensor] = None,\n","                tgt_key_padding_mask: Optional[Tensor] = None,\n","                memory_key_padding_mask: Optional[Tensor] = None,\n","                pos: Optional[Tensor] = None,\n","                query_pos: Optional[Tensor] = None):\n","        if self.normalize_before:\n","            return self.forward_pre(tgt, memory, tgt_mask, memory_mask,\n","                                    tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)\n","        return self.forward_post(tgt, memory, tgt_mask, memory_mask,\n","                                 tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)\n","\n","\n","def _get_clones(module, N):\n","    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n","\n","\n","def build_transformer(args):\n","    return Transformer(\n","        d_model=args.hidden_dim,\n","        dropout=args.dropout,\n","        nhead=args.nheads,\n","        dim_feedforward=args.dim_feedforward,\n","        num_encoder_layers=args.enc_layers,\n","        num_decoder_layers=args.dec_layers,\n","        normalize_before=args.pre_norm,\n","        return_intermediate_dec=True,\n","    )\n","\n","\n","def _get_activation_fn(activation):\n","\n","    if activation == \"relu\":\n","        return F.relu\n","    if activation == \"gelu\":\n","        return F.gelu\n","    if activation == \"glu\":\n","        return F.glu\n","    raise RuntimeError(F\"activation should be relu/gelu, not {activation}.\")"]},{"cell_type":"markdown","metadata":{"id":"6Nr2P6rvARBx"},"source":["## Positional Encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jIMwG8diAQju"},"outputs":[],"source":["import math\n","import torch\n","from torch import nn\n","\n","class NestedTensor(object):\n","    def __init__(self, tensors, mask: Optional[Tensor]):\n","        self.tensors = tensors\n","        self.mask = mask\n","\n","    def to(self, device):\n","        cast_tensor = self.tensors.to(device)\n","        mask = self.mask\n","        if mask is not None:\n","            assert mask is not None\n","            cast_mask = mask.to(device)\n","        else:\n","            cast_mask = None\n","        return NestedTensor(cast_tensor, cast_mask)\n","\n","    def decompose(self):\n","        return self.tensors, self.mask\n","\n","    def __repr__(self):\n","        return str(self.tensors)\n","\n","class PositionEmbeddingLearned(nn.Module):\n","\n","    def __init__(self, num_pos_feats=256):\n","        super().__init__()\n","        self.row_embed = nn.Embedding(50, num_pos_feats)\n","        self.col_embed = nn.Embedding(50, num_pos_feats)\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        nn.init.uniform_(self.row_embed.weight)\n","        nn.init.uniform_(self.col_embed.weight)\n","\n","    def forward(self, x):    # tensor_list: NestedTensor\n","        # x = tensor_list.tensors\n","        h, w = x.shape[-2:]\n","        i = torch.arange(w, device=x.device)\n","        j = torch.arange(h, device=x.device)\n","        x_emb = self.col_embed(i)\n","        y_emb = self.row_embed(j)\n","        pos = torch.cat([\n","            x_emb.unsqueeze(0).repeat(h, 1, 1),\n","            y_emb.unsqueeze(1).repeat(1, w, 1),\n","        ], dim=-1).permute(2, 0, 1).unsqueeze(0).repeat(x.shape[0], 1, 1, 1)\n","        return pos\n"]},{"cell_type":"markdown","metadata":{"id":"57BnuIRYsATK"},"source":["## Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"apihZMM7uatI"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H33QxR3Lw4VP"},"outputs":[],"source":["query_embed=nn.Embedding(num_embeddings=1,embedding_dim=3)\n","x_train,y_train,x_test,y_test=Data[\"vertex_train\"],Data[\"label_train\"],Data[\"vertex_test\"],Data[\"label_test\"]\n","position_embedding = PositionEmbeddingLearned()\n","model=Transformer( d_model=3, nhead=1, num_encoder_layers=6,\n","                 num_decoder_layers = 6, dim_feedforward = 3, dropout = 0.1,\n","                 activation = \"relu\")\n","param_dict=model.parameters()\n","optimizer=torch.optim.AdamW(param_dict,lr=0.001,weight_decay=0.0004)\n","loss_fn=nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ft4Jv7g9pNcm"},"outputs":[],"source":["for epoch in range(1):\n","    for x,y in zip(x_train,y_train):\n","        optimizer.zero_grad()\n","        x=torch.tensor(x[:3])\n","        # print(x.shape)\n","        x=x.view(1,x.shape[0],x.shape[1])\n","        # print(x)\n","        # positinal_encoding=position_embedding(x)\n","        positinal_encoding=None\n","        pred=model(src=x, mask=None, query_embed=query_embed.weight.unsqueeze(1), pos_embed=positinal_encoding)\n","        # pred=model(src=x, mask=None, query_embed=None, pos_embed=positinal_encoding)\n","        loss=loss_fn(pred,y)\n","        loss.backward()\n","        optimizer.step()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":669,"status":"ok","timestamp":1650107046048,"user":{"displayName":"Ravi Gautam","userId":"04156610004868127973"},"user_tz":-330},"id":"mQz4QsK0-DU2","outputId":"87e65f36-3928-4af1-fcff-9cf9fe9b3501"},"outputs":[{"data":{"text/plain":["torch.Size([1, 6, 3, 2])"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["p=PositionEmbeddingLearned(3)\n","a=torch.tensor([[[3,2],[5,4],[9,8]]])\n","k=p(a)\n","k.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1650195268253,"user":{"displayName":"Ravi Gautam","userId":"04156610004868127973"},"user_tz":-330},"id":"EbE9CjLbCAo-","outputId":"3266f6b5-526a-4768-f0d0-b4e9a0c8bc0a"},"outputs":[{"data":{"text/plain":["Parameter containing:\n","tensor([[3.7677e-04, 6.3835e-01, 7.7134e-01]], requires_grad=True)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["query_embed.weight"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TMXZE6zVCHDT"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"phaetsF35N-4"},"source":["## Graph Attention Network (GAN) Hard Coded"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4qdW_JON5SW8"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","torch.manual_seed(2020)\n","class GATLayer(nn.Module):\n","  \n","    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n","        super(GATLayer, self).__init__()\n","        self.dropout       = dropout        # drop prob = 0.6\n","        self.in_features   = in_features    \n","        self.out_features  = out_features   \n","        self.alpha         = alpha          # LeakyReLU with negative input slope, alpha = 0.2\n","        self.concat        = concat         \n","        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n","        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n","        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n","        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n","        \n","        \n","        self.leakyrelu = nn.LeakyReLU(self.alpha)\n","\n","    def forward(self, input, adj):\n","        # Linear Transformation\n","        h = torch.mm(input, self.W)\n","        N = h.size()[0]\n","\n","        # Attention Mechanism\n","        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n","        e       = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n","\n","        # Masked Attention\n","        zero_vec  = -9e15*torch.ones_like(e)\n","        attention = torch.where(adj > 0, e, zero_vec)\n","        \n","        attention = F.softmax(attention, dim=1)\n","        attention = F.dropout(attention, self.dropout, training=self.training)\n","        h_prime   = torch.matmul(attention, h)\n","\n","        if self.concat:\n","            return F.elu(h_prime)\n","        else:\n","            return h_prime\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fjNgNgbruN3v"},"source":["## Graph Attention Network (Direct Code)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7R0IgkWNuWOA"},"outputs":[],"source":["from torch_geometric.nn import GATConv\n","class GAT(torch.nn.Module):\n","    def __init__(self):\n","        super(GAT, self).__init__()\n","        self.hid = 8\n","        self.in_head = 8\n","        self.out_head = 1\n","        \n","        self.conv1 = GATConv(dataset.num_features, self.hid, heads=self.in_head, dropout=0.6)\n","        self.conv2 = GATConv(self.hid*self.in_head, dataset.num_classes, concat=False,\n","                             heads=self.out_head, dropout=0.6)\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","        x = F.dropout(x, p=0.6, training=self.training)\n","        x = self.conv1(x, edge_index)\n","        x = F.elu(x)\n","        x = F.dropout(x, p=0.6, training=self.training)\n","        x = self.conv2(x, edge_index)\n","        \n","        return F.log_softmax(x, dim=1)"]},{"cell_type":"markdown","metadata":{"id":"LiEkykpYujDO"},"source":["## Euclidean data to Graphical Data conversion"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1650215221821,"user":{"displayName":"Ravi Gautam","userId":"04156610004868127973"},"user_tz":-330},"id":"q0VF4YJ7e-Zp","outputId":"b822c783-b418-4ba7-c6a3-4dfeae5104a7"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'1.10.0+cu111'"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["torch.__version__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uKJeCchpfUKK"},"outputs":[],"source":["!pip install torch-scatter -f https://data.pyg.org/whl/torch-${'1.10.0+cu111'}+${'11.1'}.html\n","!pip install torch-sparse -f https://data.pyg.org/whl/torch-${'1.10.0+cu111'}+${'11.1'}.html\n","!pip install torch-geometric"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LWYbHQ_2-WSE"},"outputs":[],"source":["from torch_geometric.data import Data\n","from torch_geometric.nn import GATConv\n","from torch_geometric.datasets import Planetoid\n","import torch_geometric.transforms as T\n","\n","import matplotlib.pyplot as plt\n","%matplotlib notebook\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","name_data = 'Cora'\n","dataset = Planetoid(root= '/tmp/' + name_data, name = name_data)\n","dataset.transform = T.NormalizeFeatures()\n","\n","print(f\"Number of Classes in {name_data}:\", dataset.num_classes)\n","print(f\"Number of Node Features in {name_data}:\", dataset.num_node_features)\n"]},{"cell_type":"markdown","metadata":{"id":"honS91gWgioR"},"source":["## Convert Euclidean data to Graphical Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MW-R_b8OVaP6"},"outputs":[],"source":["import torch\n","import pickle\n","with open(\"/content/drive/MyDrive/Colab Notebooks/Deep Learning/GNN/modelnet10.pickle\",'rb') as data:\n","    Data=pickle.load(data)\n","x_train,y_train,x_test,y_test=Data[\"vertex_train\"],Data[\"label_train\"],Data[\"vertex_test\"],Data[\"label_test\"]\n","dist_thres=1.1  # hyperparameter\n","edge_index,Adjacency_matrix=[],[]\n","for obj in x_train:\n","    N=len(obj)\n","    obj=torch.tensor(obj)\n","    edges=[[],[]]\n","    adj=torch.ones(N,N)\n","    for i in range(N):\n","        for j in range(N):\n","            if torch.norm((obj[i]-obj[j]))<dist_thres:\n","                edges[0].append(i)\n","                edges[1].append(j)\n","            else:\n","                adj[i][j]=0\n","    edge_index.append(torch.tensor(edges))\n","    Adjacency_matrix.append(adj)\n","    break\n","\n","edge_index1,Adjacency_matrix1=[],[]\n","for obj in x_test:\n","    N=len(obj)\n","    obj=torch.tensor(obj)\n","    edges=[[],[]]\n","    adj=torch.ones(N,N)\n","    for i in range(N):\n","        for j in range(N):\n","            if torch.norm((obj[i]-obj[j]))<dist_thres:\n","                edges[0].append(i)\n","                edges[1].append(j)\n","            else:\n","                adj[i][j]=0\n","    edge_index1.append(torch.tensor(edges))\n","    Adjacency_matrix1.append(adj)\n","    break\n","# graph_data=Data(x=x_train,y=y_train,edge_index=edge_index)\n","                "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":453,"status":"ok","timestamp":1652096152149,"user":{"displayName":"Ravi Gautam","userId":"04156610004868127973"},"user_tz":-330},"id":"DRnj1G_ol62T","outputId":"305f904f-fd76-4959-f51b-d6b60e726b64"},"outputs":[{"data":{"text/plain":["50681"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["len(edge_index1[0][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vbRJM16QfoV5"},"outputs":[],"source":["import torch\n","device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"rd1sfZXzv20u"},"source":["## Architecture 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LEBXNjCcxYdy"},"outputs":[],"source":["class architecture1(torch.nn.Module):\n","    def __init__(self,input_dim=3,embed_dim=32):\n","        super().__init__()\n","        self.embed_dim=embed_dim\n","        self.input_dim=input_dim\n","        self.embed_layer=torch.nn.Linear(self.input_dim,self.embed_dim)\n","        self.object_query=torch.nn.Embedding(1,self.embed_dim)\n","        # Encoder Layer\n","        self.enc_attn_layer1=torch.nn.Linear(2*self.embed_dim,1)\n","        self.leaky_relu=torch.nn.LeakyReLU(negative_slope=0.2)\n","        self.norm_softmax1=torch.nn.Softmax(dim=1)\n","        self.enc_layer1_1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","        self.Norm=torch.nn.LayerNorm(self.embed_dim)\n","        self.relu=torch.nn.ReLU()\n","        self.enc_layer2_1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","\n","        # Decoder Layer\n","\n","\n","    def forward(self,x,adjacency_matrix,edge_index):\n","        total_edges=len(edge_index[0])\n","        \n","        # Embedding\n","        input=self.embed_layer(x)\n","\n","        # Encoder Layer\n","        enc_attn_matrix1=torch.zeros(adjacency_matrix.shape)\n","        for i in range(total_edges):\n","            node1,node2=edge_index[0][i],edge_index[1][i]\n","            node_pair=torch.cat((input[node1],input[node2]))\n","            enc_attn_matrix1[node1][node2]=self.leaky_relu1(self.attn_layer1(node_pair))\n","        enc_attn_matrix1=self.norm_softmax1(enc_attn_matrix1)\n","        attn_out=self.Norm1_1(input+torch.matmul(enc_attn_matrix1,self.relu(self.layer1_1(input))))\n","        enc_out_1=self.Norm2_1(attn_out+self.relu(self.layer2_1(input)))\n"]},{"cell_type":"markdown","metadata":{"id":"lM6uh9yfv9OJ"},"source":["## Architecture 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kk7sbxX64TrB"},"outputs":[],"source":["class architecture2(torch.nn.Module):\n","    def __init__(self,inp_dim=3,embed_dim=32,n_classes=4):\n","        super().__init__()\n","        self.n_classes=n_classes\n","        self.embed_dim=embed_dim\n","        self.inp_dim=inp_dim\n","        self.embed_layer=torch.nn.Linear(self.inp_dim,self.embed_dim)\n","        self.object_query=torch.nn.Embedding(1,self.embed_dim)\n","\n","        self.softmax=torch.nn.Softmax(dim=1)\n","        self.relu=torch.nn.ReLU()\n","        self.Norm=torch.nn.LayerNorm(self.embed_dim)\n","\n","        # Encoder Layer\n","        self.enc_key1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","        self.enc_query1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","        self.enc_value1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","        self.enc_FFN1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","\n","        # Decoder Layer\n","        self.dec_key1_1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","        self.dec_query1_1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","        self.dec_value1_1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","\n","        self.dec_key2_1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","        self.dec_value2_1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","        self.dec_FFN1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","\n","        # Final Output Layer\n","        self.out=torch.nn.Linear(self.embed_dim,self.n_classes)\n","\n","    def forward(self,x,adj):\n","        inp=self.embed_layer(x)\n","\n","        # Encoder Layer\n","        key=torch.matmul(adj,self.enc_key1(inp))\n","        query=torch.matmul(adj,self.enc_query1(inp))\n","        value=torch.matmul(adj,self.enc_value1(inp))\n","        attn=self.Norm(torch.matmul(self.softmax(torch.matmul(query,torch.t(key))),value)+inp)\n","        enc_out=self.Norm(torch.matmul(adj,self.enc_FFN1(attn))+attn)\n","\n","        # Decoder Layer\n","        key=self.dec_key1_1(self.object_query.weight)\n","        query=self.dec_query1_1(self.object_query.weight)\n","        value=self.dec_value1_1(self.object_query.weight)\n","        Q=self.Norm(torch.matmul(self.softmax(torch.matmul(query,torch.t(key))),value)+self.object_query.weight)\n","        key=torch.matmul(adj,self.dec_key2_1(enc_out))\n","        value=torch.matmul(adj,self.dec_value2_1(enc_out))\n","        attn=self.Norm(torch.matmul(self.softmax(torch.matmul(Q,torch.t(key))),value)+Q)\n","        dec_out=self.Norm(self.dec_FFN1(attn)+attn)\n","\n","        # Final Output Layer\n","        output=torch.nn.functional.softmax(self.out(dec_out))\n","\n","        return output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1651499827879,"user":{"displayName":"Ravi Gautam","userId":"04156610004868127973"},"user_tz":-330},"id":"NPxfzd2-DVvJ","outputId":"35f64e06-7a22-437d-d8a5-9db51b32b0a7"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"]}],"source":["model=architecture2().to(device)\n","opt=torch.optim.Adam(model.parameters(),lr=0.01)\n","loss_fn=torch.nn.CrossEntropyLoss()\n","for epoch in range(25):\n","    for x,y,adj in zip(x_train,y_train,Adjacency_matrix):\n","        x,y,adj=torch.tensor(x).to(device),torch.tensor([y],dtype=torch.int64).to(device),adj.to(device)\n","        opt.zero_grad()\n","        pred=model(x,adj)\n","        loss=loss_fn(pred,y)\n","        loss.backward()\n","        opt.step()\n","        break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4_tYq91tEARg"},"outputs":[],"source":["output=[]\n","for x,y,adj in zip(x_test,y_test,Adjacency_matrix1):\n","    x,y,adj=torch.tensor(x).to(device),torch.tensor([y],dtype=torch.int64).to(device),adj.to(device)\n","    pred=model(x,adj)\n","    output.append(torch.argmax(pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ecf8J0OnWgVI"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3422,"status":"ok","timestamp":1653065637654,"user":{"displayName":"Ravi Gautam","userId":"04156610004868127973"},"user_tz":-330},"id":"hwsA_8DMWf7R","outputId":"fd48aeeb-8a35-4ab9-e2d6-a1ff8da89368"},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import pickle\n","from numpy.random import randint\n","device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2hJIZphfWiVd"},"outputs":[],"source":["class architecture2(torch.nn.Module):\n","    def __init__(self,inp_dim=3,embed_dim=32,n_classes=4):\n","        super().__init__()\n","        self.n_classes=n_classes\n","        self.embed_dim=embed_dim\n","        self.inp_dim=inp_dim\n","        self.embed_layer=torch.nn.Linear(self.inp_dim,self.embed_dim)\n","        self.object_query=torch.nn.Embedding(1,self.embed_dim)\n","\n","        self.softmax=torch.nn.Softmax(dim=1)\n","        self.relu=torch.nn.ReLU()\n","        self.Norm=torch.nn.LayerNorm(self.embed_dim)\n","\n","        # Encoder Layer\n","        self.enc_key1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","        self.enc_query1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","        self.enc_value1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","        self.enc_FFN1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","\n","        # Decoder Layer\n","        self.dec_key1_1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","        self.dec_query1_1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","        self.dec_value1_1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","\n","        self.dec_key2_1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","        self.dec_value2_1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","        self.dec_FFN1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","\n","        # Final Output Layer\n","        self.out=torch.nn.Linear(self.embed_dim,self.n_classes)\n","\n","    def forward(self,x):\n","        inp=self.embed_layer(x)\n","\n","        # Encoder Layer\n","        key=self.enc_key1(inp)\n","        query=self.enc_query1(inp)\n","        value=self.enc_value1(inp)\n","        attn=self.Norm(torch.matmul(self.softmax(torch.matmul(query,torch.t(key))),value)+inp)\n","        enc_out=self.Norm(self.enc_FFN1(attn)+attn)\n","\n","        # Decoder Layer\n","        key=self.dec_key1_1(self.object_query.weight)\n","        query=self.dec_query1_1(self.object_query.weight)\n","        value=self.dec_value1_1(self.object_query.weight)\n","        Q=self.Norm(torch.matmul(self.softmax(torch.matmul(query,torch.t(key))),value)+self.object_query.weight)\n","        key=self.dec_key2_1(enc_out)\n","        value=self.dec_value2_1(enc_out)\n","        attn=self.Norm(torch.matmul(self.softmax(torch.matmul(Q,torch.t(key))),value)+Q)\n","        dec_out=self.Norm(self.dec_FFN1(attn)+attn)\n","\n","        # Final Output Layer\n","        output=torch.nn.functional.softmax(self.out(dec_out))\n","\n","        return output\n","\n","\n","model=architecture2().to(device)\n","opt=torch.optim.Adam(model.parameters(),lr=0.01)\n","loss_fn=torch.nn.CrossEntropyLoss()\n","with open(\"/content/drive/MyDrive/Colab Notebooks/Deep Learning/GNN/modelnet10.pickle\",'rb') as data:\n","    Data=pickle.load(data)\n","x_train,y_train,x_test,y_test=Data[\"vertex_train\"],Data[\"label_train\"],Data[\"vertex_test\"],Data[\"label_test\"]\n","dist_thres=1.1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":593358,"status":"ok","timestamp":1653067353671,"user":{"displayName":"Ravi Gautam","userId":"04156610004868127973"},"user_tz":-330},"id":"Nkuvvd96mpnS","outputId":"b86a1b3d-907c-4aa7-afc4-aaa5a7e6ffc2"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy:  30.386740331491712\n"]}],"source":["max_size=5000\n","for epoch in range(100000):\n","    # for x,y in zip(x_train,y_train):\n","    idx=randint(0,len(x_train))\n","        # x,y=torch.tensor(x).to(device),torch.tensor([y],dtype=torch.int64).to(device)\n","    x,y=torch.tensor(x_train[idx]).to(device),torch.tensor([y_train[idx]],dtype=torch.int64).to(device)\n","    if len(x)<max_size:\n","        opt.zero_grad()\n","        pred=model(x)\n","        loss=loss_fn(pred,y)\n","        loss.backward()\n","        opt.step()\n","\n","correct,total=0,0\n","for epoch in range(250):\n","    idx=randint(0,len(x_test))\n","    x=torch.tensor(x_test[idx]).to(device)\n","    if len(x)<max_size:\n","        pred=torch.argmax(model(x))\n","        total+=1\n","        if y_test[idx]==pred:\n","            correct+=1\n","accuracy=correct*100/total\n","print(\"Accuracy: \",accuracy)"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3178,"status":"ok","timestamp":1653453688601,"user":{"displayName":"Ravi Gautam","userId":"04156610004868127973"},"user_tz":-330},"id":"NHheO2QjdDCD"},"outputs":[],"source":["import torch\n","import pickle\n","from numpy.random import randint\n","device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","class architecture2(torch.nn.Module):\n","    def __init__(self,inp_dim=3,embed_dim=64,n_classes=4):\n","        super().__init__()\n","        self.n_classes=n_classes\n","        self.embed_dim=embed_dim\n","        self.inp_dim=inp_dim\n","        self.embed_layer=torch.nn.Linear(self.inp_dim,self.embed_dim)\n","        self.object_query=torch.nn.Embedding(1,self.embed_dim)\n","\n","        self.softmax=torch.nn.Softmax(dim=1)\n","        self.relu=torch.nn.ReLU()\n","        self.Norm=torch.nn.LayerNorm(self.embed_dim)\n","\n","        # Encoder Layer\n","        self.enc_key1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","        self.enc_query1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","        self.enc_value1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","        self.enc_FFN1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","\n","        # Decoder Layer\n","        self.dec_key1_1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","        self.dec_query1_1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","        self.dec_value1_1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","\n","        self.dec_key2_1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","        self.dec_value2_1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","        self.dec_FFN1=torch.nn.Linear(self.embed_dim,self.embed_dim)\n","\n","        # Final Output Layer\n","        self.out=torch.nn.Linear(self.embed_dim,self.n_classes)\n","\n","    def forward(self,x,adj):\n","        inp=self.embed_layer(x)\n","\n","        # Encoder Layer\n","        key=torch.matmul(adj,self.enc_key1(inp))\n","        query=torch.matmul(adj,self.enc_query1(inp))\n","        value=torch.matmul(adj,self.enc_value1(inp))\n","        attn=self.Norm(torch.matmul(self.softmax(torch.matmul(query,torch.t(key))),value)+inp)\n","        enc_out=self.Norm(torch.matmul(adj,self.enc_FFN1(attn))+attn)\n","\n","        # Decoder Layer\n","        key=self.dec_key1_1(self.object_query.weight)\n","        query=self.dec_query1_1(self.object_query.weight)\n","        value=self.dec_value1_1(self.object_query.weight)\n","        Q=self.Norm(torch.matmul(self.softmax(torch.matmul(query,torch.t(key))),value)+self.object_query.weight)\n","        key=torch.matmul(adj,self.dec_key2_1(enc_out))\n","        value=torch.matmul(adj,self.dec_value2_1(enc_out))\n","        attn=self.Norm(torch.matmul(self.softmax(torch.matmul(Q,torch.t(key))),value)+Q)\n","        dec_out=self.Norm(self.dec_FFN1(attn)+attn)\n","\n","        # Final Output Layer\n","        output=torch.nn.functional.softmax(self.out(dec_out))\n","\n","        return output"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":25329,"status":"ok","timestamp":1653453714387,"user":{"displayName":"Ravi Gautam","userId":"04156610004868127973"},"user_tz":-330},"id":"dBFc31INthmu"},"outputs":[],"source":["model=architecture2().to(device)\n","opt=torch.optim.Adam(model.parameters(),lr=0.01,weight_decay=0.007)\n","loss_fn=torch.nn.CrossEntropyLoss()\n","with open(\"/content/drive/MyDrive/Colab Notebooks/Deep Learning/GNN/modelnet10.pickle\",'rb') as data:\n","    Data=pickle.load(data)\n","x_train,y_train=Data[\"vertex_train\"],Data[\"label_train\"]\n","with open(\"/content/drive/MyDrive/Colab Notebooks/Deep Learning/GNN/test_data.pickle\",'rb') as data:\n","    Data=pickle.load(data)\n","x_test,y_test=Data[\"vertex_test\"],Data[\"label_test\"]\n","dist_thres=0.5\n","min_points=1000"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rogea2p-tSwR","executionInfo":{"status":"ok","timestamp":1653459728664,"user_tz":-330,"elapsed":6014281,"user":{"displayName":"Ravi Gautam","userId":"04156610004868127973"}},"outputId":"6ed62d0b-6120-4f9f-8a0f-bfb4e05cf0e9"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy:  31.25\n"]}],"source":["for epoch in range(3000):\n","    # for x,y in zip(x_train,y_train):\n","    idx=randint(0,len(x_train))\n","        # x,y=torch.tensor(x).to(device),torch.tensor([y],dtype=torch.int64).to(device)\n","    x,y=torch.tensor(x_train[idx]).to(device),torch.tensor([y_train[idx]],dtype=torch.int64).to(device)\n","    N=len(x)\n","    if N<min_points:\n","        adj=torch.ones(N,N).to(device)\n","        for i in range(N):\n","            for j in range(i+1,N):\n","                if torch.norm(x[i]-x[j])>dist_thres:\n","                    adj[i][j]=0\n","                    adj[j][i]=0\n","\n","\n","        opt.zero_grad()\n","        pred=model(x,adj)\n","        loss=loss_fn(pred,y)\n","        loss.backward()\n","        opt.step()\n","#         print(\"epoch: \",epoch)\n","    \n","correct=0\n","total=0\n","for epoch in range(250):\n","    idx=randint(0,len(x_test))\n","    x=torch.tensor(x_test[idx]).to(device)\n","    N=len(x)\n","    if N<min_points:\n","        adj=torch.ones(N,N).to(device)\n","        for i in range(N):\n","            for j in range(i+1,N):\n","                if torch.norm(x[i]-x[j])>dist_thres:\n","                    adj[i][j]=0\n","                    adj[j][i]=0\n","        pred=torch.argmax(model(x,adj))\n","        total+=1\n","        if y_test[idx]==pred:\n","            correct+=1\n","        \n","accuracy=correct*100/total\n","print(\"Accuracy: \",accuracy)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":613,"status":"ok","timestamp":1653400999419,"user":{"displayName":"Ravi Gautam","userId":"04156610004868127973"},"user_tz":-330},"id":"dwfofQf524qG","outputId":"dee65b21-cb44-4305-d525-8016656b2958"},"outputs":[{"data":{"text/plain":["device(type='cpu')"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gl2AG0RcYQ9U"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["iKd3yMJh2DNN","6Nr2P6rvARBx","phaetsF35N-4","fjNgNgbruN3v","LiEkykpYujDO","rd1sfZXzv20u"],"name":"point_proj.ipynb","provenance":[],"mount_file_id":"1NQ5glHj5zB6-v3Q_9SBbmwxmvf_KOlac","authorship_tag":"ABX9TyPbpyegqhdMvqJ3/I+VPzWl"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}